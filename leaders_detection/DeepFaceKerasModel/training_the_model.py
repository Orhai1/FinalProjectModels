# -*- coding: utf-8 -*-
"""deepface_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dj53jsDouZei0xYbHqyNQsC0xCMnChT
"""

# !pip uninstall -y keras tensorflow
#
# !pip uninstall -y keras tensorflow
# !pip install keras==2.12.0 tensorflow==2.12.0
#
# !pip install deepface

from google.colab import drive
drive.mount('/content/drive')

train_dir = "/content/drive/MyDrive/training_set/training_set"

test_dir = "/content/drive/MyDrive/test_set/test_set"

import os

print("Train folders:", os.listdir(train_dir))
print("Test folders:", os.listdir(test_dir))


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import load_model

from deepface import DeepFace
import os
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
import joblib
from google.colab import files
from sklearn.metrics import precision_score, recall_score, f1_score

# Run this in Google Colab
model_name = "Facenet"

# STEP 1: Extract training embeddings
X, y = [], []
for person in os.listdir(train_dir):
    person_dir = os.path.join(train_dir, person)
    if not os.path.isdir(person_dir): continue
    for img_name in os.listdir(person_dir):
        img_path = os.path.join(person_dir, img_name)
        print(f"üîç Processing: {img_path}")  # Add this
        try:
            embedding = DeepFace.represent(
                img_path=img_path,
                model_name="Facenet",  # or use "SFace"
                enforce_detection=False
            )[0]["embedding"]
            X.append(embedding)
            y.append(person.lower())
        except Exception as e:
            print(f"‚ùå Skipped: {img_path} | Reason: {e}")


print(f"\nCollected {len(X)} embeddings from {len(set(y))} people.")

# STEP 2: Preprocess
X = np.array(X)
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_scaled = (X - X_mean) / X_std

# Save mean and std for later (optional)
np.save("scaler_mean.npy", X_mean)
np.save("scaler_std.npy", X_std)

# Manual label encoder
class_names = sorted(list(set(y)))
y_encoded = np.array([class_names.index(label) for label in y])


# STEP 3: Extract test embeddings
X_test, y_test = [], []
print("\nüß™ Extracting test embeddings...")
for person in os.listdir(test_dir):
    person_dir = os.path.join(test_dir, person)
    if not os.path.isdir(person_dir): continue
    for img_name in os.listdir(person_dir):
        img_path = os.path.join(person_dir, img_name)
        try:
            emb = DeepFace.represent(img_path=img_path, model_name=model_name, enforce_detection=False)[0]["embedding"]
            X_test.append(emb)
            y_test.append(person.lower())
        except:
            print(f"Failed: {img_path}")

X_test = np.array(X_test)
X_test_scaled = (X_test - X_mean) / X_std
y_test_encoded = np.array([class_names.index(label) for label in y_test])


# STEP 4: Train the Models
# One-hot encode labels for Keras softmax classifier
y_categorical = to_categorical(y_encoded)
y_test_categorical = to_categorical(y_test_encoded)

# Define Keras model
model = Sequential([
    Input(shape=(X_scaled.shape[1],)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(y_categorical.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ‚úÖ Sanity checks BEFORE training
print("‚úÖ Training embeddings shape:", X_scaled.shape)
print("‚úÖ Test embeddings shape:", X_test_scaled.shape)

# Train model
model.fit(X_scaled, y_categorical, epochs=25, batch_size=16, validation_split=0.1)

# Evaluate on test data
loss, accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=0)

# Predict for metrics
y_pred_probs = model.predict(X_test_scaled)
y_pred = np.argmax(y_pred_probs, axis=1)

# Decode predicted and actual labels (optional, for inspection)
for i in range(len(y_pred)):
    predicted_label = class_names[y_pred[i]]
    actual_label = class_names[y_test_encoded[i]]
    print(f" Prediction: {predicted_label} | Actual: {actual_label}")


# Metrics
prec = precision_score(y_test_encoded, y_pred, average='weighted', zero_division=0)
rec = recall_score(y_test_encoded, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test_encoded, y_pred, average='weighted', zero_division=0)

print(f"\nKeras NN ‚Üí Accuracy: {accuracy*100:.2f}%, Precision: {prec:.2f}, Recall: {rec:.2f}, F1: {f1:.2f}")




# STEP 5: Save results
results = [{
    "Model": "Keras_NN",
    "Accuracy (%)": round(accuracy * 100, 2),
    "Precision (%)": round(prec * 100, 2),
    "Recall (%)": round(rec * 100, 2),
    "F1 Score (%)": round(f1 * 100, 2)
}]

df = pd.DataFrame(results)
df.to_csv("model_comparison_results1.csv", index=False)
files.download("model_comparison_results1.csv")

# Save the trained Keras model
model.save("face_keras_model.h5")
model.save("face_keras_model.keras")

# Download it to your local machine (works in Google Colab)
files.download("face_keras_model.h5")
files.download("face_keras_model.keras")

import json
with open("class_names.json", "w") as f:
    json.dump(class_names, f)

files.download("scaler_mean.npy")
files.download("scaler_std.npy")
files.download("class_names.json")

import matplotlib.pyplot as plt

metrics = {
    'Accuracy': 0.6920,
    'Precision': 0.75,
    'Recall': 0.69,
    'F1 Score': 0.70
}

plt.figure(figsize=(8, 6))
plt.bar(metrics.keys(), metrics.values(), color='skyblue')
plt.ylim(0, 1)
plt.title('Keras NN Performance Metrics')
plt.ylabel('Score')
plt.grid(axis='y', linestyle='--', alpha=0.7)

for i, (metric, value) in enumerate(metrics.items()):
    plt.text(i, value + 0.02, f'{value:.2f}', ha='center', fontsize=12)

plt.show()