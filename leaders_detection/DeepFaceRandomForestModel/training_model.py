# -*- coding: utf-8 -*-
"""deepface_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dj53jsDouZei0xYbHqyNQsC0xCMnChT
"""

# !pip uninstall -y keras tensorflow
#
# !pip uninstall -y keras tensorflow
# !pip install keras==2.12.0 tensorflow==2.12.0

# !pip install deepface

# from google.colab import drive
# drive.mount('/content/drive')

train_dir = "/content/drive/MyDrive/training_set"
test_dir = "/content/drive/MyDrive/test_set"

import os
print(os.listdir(train_dir))  # Should show leader folders like AbuMazen, Sinwar, etc.
print(os.listdir(test_dir))

import tensorflow as tf
print("GPU available:" if tf.config.list_physical_devices('GPU') else "GPU not available")



# Run this in Google Colab after uploading and unzipping folders manually

from deepface import DeepFace
import os
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
import joblib
from google.colab import files
from sklearn.metrics import precision_score, recall_score, f1_score

# ✅ Updated Paths
train_dir = "training_set/augmented_photos"
test_dir = "test_set"
model_name = "ArcFace"

# STEP 1: Extract training embeddings
X, y = [], []
for person in os.listdir(train_dir):
    person_dir = os.path.join(train_dir, person)
    if not os.path.isdir(person_dir): continue
    for img_name in os.listdir(person_dir):
        img_path = os.path.join(person_dir, img_name)
        print(f"Processing: {img_path}")  # Add this
        try:
            embedding = DeepFace.represent(
                img_path=img_path,
                model_name="ArcFace",
                enforce_detection=False
            )[0]["embedding"]
            X.append(embedding)
            y.append(person.lower())
        except Exception as e:
            print(f"Skipped: {img_path} | Reason: {e}")


print(f"\nCollected {len(X)} embeddings from {len(set(y))} people.")

# STEP 2: Preprocess
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# STEP 3: Extract test embeddings
X_test, y_test = [], []
print("\nExtracting test embeddings...")
for person in os.listdir(test_dir):
    person_dir = os.path.join(test_dir, person)
    if not os.path.isdir(person_dir): continue
    for img_name in os.listdir(person_dir):
        img_path = os.path.join(person_dir, img_name)
        try:
            emb = DeepFace.represent(img_path=img_path, model_name=model_name, enforce_detection=False)[0]["embedding"]
            X_test.append(emb)
            y_test.append(person.lower())
        except:
            print(f"Failed: {img_path}")

X_test_scaled = scaler.transform(X_test)
y_test_encoded = le.transform(y_test)

# STEP 4: Train & Compare Models
models = {
    "SVM": SVC(kernel="linear", probability=True),
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=3),
    "LogisticRegression": LogisticRegression(max_iter=1000)
}

results = []
for name, model in models.items():
    model.fit(X_scaled, y_encoded)
    preds = model.predict(X_test_scaled)
    acc = accuracy_score(y_test_encoded, preds)

    #new
    prec = precision_score(y_test_encoded, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test_encoded, preds, average='weighted', zero_division=0)
    f1 = f1_score(y_test_encoded, preds, average='weighted', zero_division=0)
    print(f"{name} → Accuracy: {acc*100:.2f}%, Precision: {prec:.2f}, Recall: {rec:.2f}, F1: {f1:.2f}")

    results.append({
        "Model": name,
        "Accuracy (%)": round(acc * 100, 2),
        "Precision (%)": round(prec * 100, 2),
        "Recall (%)": round(rec * 100, 2),
        "F1 Score (%)": round(f1 * 100, 2)
    })


# STEP 5: Save results
df = pd.DataFrame(results).sort_values("Accuracy (%)", ascending=False)
df.to_csv("model_comparison_results1.csv", index=False)
files.download("model_comparison_results1.csv")

# Save best model (RandomForest), scaler, and label encoder
joblib.dump(models["RandomForest"], "best_model_randomforest1.pkl")
joblib.dump(scaler, "scaler1.pkl")
joblib.dump(le, "label_encoder1.pkl")

# Download the files to your computer
files.download("best_model_randomforest1.pkl")
files.download("scaler1.pkl")
files.download("label_encoder1.pkl")

import pandas as pd
import matplotlib.pyplot as plt


#Print diagrams of the results of the model

#Print a model comparison
# Load the model results from the CSV
df = pd.read_csv("model_comparison_results1.csv")

# Set figure size
plt.figure(figsize=(10, 6))

# Metrics to plot
metrics = ["Accuracy (%)", "Precision (%)", "Recall (%)", "F1 Score (%)"]
bar_width = 0.2
x = range(len(df["Model"]))

# Plot each metric
for i, metric in enumerate(metrics):
    plt.bar(
        [pos + i * bar_width for pos in x],
        df[metric],
        width=bar_width,
        label=metric
    )

# X-axis settings
plt.xticks([pos + bar_width * 1.5 for pos in x], df["Model"])
plt.xlabel("Model")
plt.ylabel("Score (%)")
plt.title("Model Performance Comparison")
plt.ylim(0, 100)
plt.legend()
plt.tight_layout()
plt.show()


#Print the best model - comparing the leaders
from sklearn.metrics import precision_recall_fscore_support
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt

# Set the model name for clarity
model_name = "RandomForest_v1"

# Get predictions from the model
rf_model = models["RandomForest"]
rf_preds = rf_model.predict(X_test_scaled)
rf_names = le.inverse_transform(rf_preds)
true_names = le.inverse_transform(y_test_encoded)
unique_people = sorted(set(true_names))

# Calculate per-person accuracy
correct_counts = defaultdict(int)
total_counts = defaultdict(int)
for true, pred in zip(true_names, rf_names):
    total_counts[true] += 1
    if true == pred:
        correct_counts[true] += 1
accuracies = [100 * correct_counts[p] / total_counts[p] for p in unique_people]

# Calculate precision, recall, F1 score
precision, recall, f1, _ = precision_recall_fscore_support(
    true_names, rf_names, labels=unique_people, zero_division=0
)

# Build metrics DataFrame
metrics_df = pd.DataFrame({
    "Person": unique_people,
    "Accuracy (%)": accuracies,
    "Precision (%)": precision * 100,
    "Recall (%)": recall * 100,
    "F1 Score (%)": f1 * 100
})

# Save to versioned CSV
csv_filename = f"{model_name.lower()}_person_metrics.csv"
metrics_df.to_csv(csv_filename, index=False)

# Plot grouped bar chart
x = range(len(unique_people))
width = 0.2

plt.figure(figsize=(12, 6))
plt.bar([i - 1.5*width for i in x], metrics_df["Accuracy (%)"], width=width, label='Accuracy', color='gray')
plt.bar([i - 0.5*width for i in x], metrics_df["Precision (%)"], width=width, label='Precision', color='orange')
plt.bar([i + 0.5*width for i in x], metrics_df["Recall (%)"], width=width, label='Recall', color='blue')
plt.bar([i + 1.5*width for i in x], metrics_df["F1 Score (%)"], width=width, label='F1 Score', color='green')

plt.xticks(ticks=x, labels=unique_people)
plt.xlabel("Person")
plt.ylabel("Metric (%)")
plt.title(f"{model_name} Metrics per Person (Accuracy, Precision, Recall, F1)")
plt.ylim(0, 100)
plt.legend()
plt.tight_layout()
plt.show()